#!/usr/bin/env python3
# lean-doc - tiny CLI browser for Lean/mathlib docs (stdlib-only)
# Usage:
#   lean-doc search [--site mathlib|lean|docs|zulip] <query>
#   lean-doc open <url-or-index>
#   lean-doc show <url>
#   lean-doc module <Mathlib/Data/Finsupp/Basic or Module.Name>
#   lean-doc latest

import os, sys, json, time, urllib.parse, urllib.request, webbrowser, html
from html.parser import HTMLParser
from xml.etree import ElementTree as ET

CACHE_DIR = os.path.join(os.path.expanduser("~"), ".cache", "lean_doc")
CACHE_FILE = os.path.join(CACHE_DIR, "cache.json")
USER_AGENT = "lean-doc/0.1"
DEFAULT_TTL = 7 * 24 * 3600  # 7 days

SITES = {
    "mathlib": "site:leanprover-community.github.io mathlib4_docs",
    "lean": "site:lean-lang.org",
    "zulip": "site:leanprover.zulipchat.com",
    "docs": "site:leanprover-community.github.io",
}

def ensure_cache():
    os.makedirs(CACHE_DIR, exist_ok=True)
    if not os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump({"search": {}, "pages": {}, "rss": {}}, f)

def load_cache():
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        return json.load(f)

def save_cache(cache):
    tmp = CACHE_FILE + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(cache, f, ensure_ascii=False, indent=2)
    os.replace(tmp, CACHE_FILE)

def http_get(url, timeout=15):
    req = urllib.request.Request(url, headers={"User-Agent": USER_AGENT})
    with urllib.request.urlopen(req, timeout=timeout) as resp:
        return resp.read()

class DuckParser(HTMLParser):
    def __init__(self):
        super().__init__()
        self.links = []
        self._capture = False
        self._text = ""
        self._current = None

    def handle_starttag(self, tag, attrs):
        attrs = dict(attrs)
        if tag == "a" and "result__a" in attrs.get("class", ""):
            self._capture = True
            self._current = {"title": "", "url": attrs.get("href", "")}

    def handle_endtag(self, tag):
        if self._capture and tag == "a":
            self._current["title"] = self._text.strip()
            if self._current.get("url"):
                self.links.append(self._current)
            self._capture = False
            self._text = ""
            self._current = None

    def handle_data(self, data):
        if self._capture:
            self._text += data

def duck_search(query, max_results=10):
    q = urllib.parse.quote_plus(query)
    url = f"https://duckduckgo.com/html/?q={q}"
    html_bytes = http_get(url)
    parser = DuckParser()
    parser.feed(html_bytes.decode("utf-8", errors="ignore"))
    cleaned = []
    for item in parser.links:
        href = item["url"]
        if href.startswith("/l/?") or href.startswith("/?q="):
            parsed = urllib.parse.urlparse(href)
            qs = urllib.parse.parse_qs(parsed.query)
            if "uddg" in qs:
                href = urllib.parse.unquote(qs["uddg"][0])
        cleaned.append({"title": html.unescape(item["title"]), "url": href})
        if len(cleaned) >= max_results:
            break
    return cleaned

def cache_key(prefix, key):
    return f"{prefix}:{key}"

def search(query, site=None, ttl=DEFAULT_TTL):
    cache = load_cache()
    filt = SITES.get(site, "")
    full_q = f"{filt} {query}".strip()
    key = cache_key("search", full_q)
    now = time.time()
    if key in cache["search"] and now - cache["search"][key]["ts"] < ttl:
        return cache["search"][key]["data"]
    results = duck_search(full_q)
    cache["search"][key] = {"ts": now, "data": results}
    save_cache(cache)
    return results

class TextExtractor(HTMLParser):
    def __init__(self, max_chars=1000):
        super().__init__()
        self.out = []
        self.max_chars = max_chars
        self._in_script = False
        self._in_style = False

    def handle_starttag(self, tag, attrs):
        if tag == "script": self._in_script = True
        if tag == "style": self._in_style = True

    def handle_endtag(self, tag):
        if tag == "script": self._in_script = False
        if tag == "style": self._in_style = False

    def handle_data(self, data):
        if self._in_script or self._in_style: return
        if data.strip():
            self.out.append(data.strip())
            if sum(len(s) for s in self.out) > self.max_chars:
                raise StopIteration

def fetch_text(url, ttl=DEFAULT_TTL):
    cache = load_cache()
    key = cache_key("page", url)
    now = time.time()
    if key in cache["pages"] and now - cache["pages"][key]["ts"] < ttl:
        return cache["pages"][key]["data"]
    raw = http_get(url)
    try:
        parser = TextExtractor()
        parser.feed(raw.decode("utf-8", errors="ignore"))
        text = " ".join(parser.out)[:1500]
    except StopIteration:
        text = " ".join(parser.out)[:1500]
    cache["pages"][key] = {"ts": now, "data": text}
    save_cache(cache)
    return text

def cmd_search(args):
    site = None
    if "--site" in args:
        i = args.index("--site")
        if i + 1 < len(args):
            site = args[i+1]
            del args[i:i+2]
    query = " ".join(args) if args else ""
    if not query:
        print("Usage: lean-doc search [--site mathlib|lean|docs|zulip] <query>")
        return 2
    res = search(query, site=site)
    if not res:
        print("No results.")
        return 1
    for i, r in enumerate(res, 1):
        print(f"[{i}] {r['title']}\n    {r['url']}")
    return 0

def cmd_open(args):
    if not args:
        print("Usage: lean-doc open <url-or-index-from-last-search>")
        return 2
    target = args[0]
    if target.isdigit():
        cache = load_cache()
        latest = None
        latest_ts = -1
        for k, v in cache["search"].items():
            if v["ts"] > latest_ts:
                latest = v
                latest_ts = v["ts"]
        if not latest:
            print("No cached search results.")
            return 1
        idx = int(target) - 1
        if idx < 0 or idx >= len(latest['data']):
            print("Index out of range.")
            return 1
        url = latest["data"][idx]["url"]
    else:
        url = target
    print(url)
    try:
        webbrowser.open(url)
    except Exception:
        pass
    return 0

def cmd_show(args):
    if not args:
        print("Usage: lean-doc show <url>")
        return 2
    url = args[0]
    text = fetch_text(url)
    print(text)
    return 0

def cmd_module(args):
    if not args:
        print("Usage: lean-doc module <Mathlib/Data/Finsupp/Basic or Module.Name>")
        return 2
    mod = args[0]
    mod = mod.replace(".", "/")
    if not mod.endswith(".html"):
        mod = mod + ".html"
    url = f"https://leanprover-community.github.io/mathlib4_docs/{mod}"
    print(url)
    try:
        webbrowser.open(url)
    except Exception:
        pass
    return 0

def fetch_rss(url, ttl=3600):
    cache = load_cache()
    key = cache_key("rss", url)
    now = time.time()
    if key in cache["rss"] and now - cache["rss"][key]["ts"] < ttl:
        return cache["rss"][key]["data"]
    raw = http_get(url)
    root = ET.fromstring(raw)
    items = []
    for item in root.findall(".//item"):
        title = item.findtext("title") or ""
        link = item.findtext("link") or ""
        pub = item.findtext("pubDate") or ""
        items.append({"title": title.strip(), "link": link.strip(), "pubDate": pub.strip()})
        if len(items) >= 10:
            break
    cache["rss"][key] = {"ts": now, "data": items}
    save_cache(cache)
    return items

def cmd_latest(args):
    feeds = [
        ("Lean Blog", "https://leanprover-community.github.io/blog/atom.xml"),
        ("Lean Lang", "https://lean-lang.org/feed.xml"),
    ]
    for name, url in feeds:
        try:
            items = fetch_rss(url)
            print(f"== {name} ==")
            for it in items[:5]:
                print(f"- {it['title']}")
                print(f"  {it['link']} ({it['pubDate']})")
        except Exception as e:
            print(f"!! Failed {name}: {e}")
    return 0

USAGE = """lean-doc - tiny CLI browser for Lean/mathlib docs (stdlib-only)

Usage:
  lean-doc search [--site mathlib|lean|docs|zulip] <query>
  lean-doc open <url-or-index>
  lean-doc show <url>
  lean-doc module <Mathlib/Data/Finsupp/Basic or Module.Name>
  lean-doc latest
"""

def main():
    ensure_cache()
    argv = sys.argv[1:]
    if not argv or argv[0] in ("-h", "--help", "help"):
        print(USAGE)
        sys.exit(0)
    cmd = argv[0]
    args = argv[1:]
    if cmd == "search":
        sys.exit(cmd_search(args))
    if cmd == "open":
        sys.exit(cmd_open(args))
    if cmd == "show":
        sys.exit(cmd_show(args))
    if cmd == "module":
        sys.exit(cmd_module(args))
    if cmd == "latest":
        sys.exit(cmd_latest(args))
    print(USAGE)
    sys.exit(2)

if __name__ == "__main__":
    main()
